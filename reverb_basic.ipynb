{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据pos_tag及ner\n",
    "每个token的范围是原句子中位置的左闭右开区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"zh_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"zh_core_web_sm\")\n",
    "#sentences = [\"物质决定意识，意识反作用于物质。\",\"清华大学位于海淀区五道口。\"]\n",
    "\n",
    "# pos_tag后的句子\n",
    "def pos_tag_sentences(sentences):\n",
    "    pos_sentences = []\n",
    "    for string in sentences:\n",
    "        doc = nlp(string)\n",
    "        pos_string = []\n",
    "        pos_sentences.append(pos_string)\n",
    "        i = 0\n",
    "        for token in doc:\n",
    "            token_length = len(token.text)\n",
    "            token_start = i\n",
    "            token_end = i+token_length\n",
    "            i = token_end\n",
    "\n",
    "            pos_string.append((token.text, token.tag_+\"_pos\", (token_start, token_end)))\n",
    "    return pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner后的句子\n",
    "def ner_tag_sentences(sentences):\n",
    "    ner_sentences = []\n",
    "    for string in sentences:\n",
    "        doc = nlp(string)\n",
    "        ner_string = []\n",
    "        ner_sentences.append(ner_string)\n",
    "        for ent in doc.ents:\n",
    "            ner_string.append((ent.text, ent.label_, (ent.start_char, ent.end_char)))\n",
    "            # print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    return ner_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用pos_tag和ner找到实体区间\n",
    "区间范围为原句子中字的位置的左闭右开区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实体抽取，需要记录词语和起止位置\n",
    "def find_argument(pos_sentences, ner_sentences):\n",
    "    argument_list = []\n",
    "    if len(pos_sentences) != 0:\n",
    "        for pos_token in pos_sentences:\n",
    "            if pos_token[1] == \"NN_pos\" or pos_token[1] == \"NR_pos\" or pos_token[1] == \"NT_pos\":\n",
    "                argument_list.append(pos_token[2])\n",
    "    \n",
    "    if len(ner_sentences) != 0:\n",
    "        for ner_token in ner_sentences:\n",
    "            argument_list.append(ner_token[2])\n",
    "    print(argument_list)\n",
    "    \n",
    "    # 如果两个argument区域相交，合并为一个\n",
    "    unified_argument = []\n",
    "    for i in range(len(argument_list)):\n",
    "        if argument_list[i][0] == (-1,-1):\n",
    "            continue\n",
    "        for j in range(i+1,len(argument_list)):\n",
    "            if argument_list[j] == (-1,-1):\n",
    "                continue\n",
    "            if (argument_list[i][1] >= argument_list[j][0] and argument_list[i][0] < argument_list[j][1])\\\n",
    "                or (argument_list[j][1] >= argument_list[i][0] and argument_list[j][0] < argument_list[i][1]):\n",
    "                argument_list[j] = (min(argument_list[i][0], argument_list[j][0]), max(argument_list[i][1], argument_list[j][1]))\n",
    "                argument_list[i] = (-1,-1)\n",
    "                break\n",
    "        if argument_list[i][0] != -1 and argument_list[i][1] != -1:\n",
    "            unified_argument.append(argument_list[i])\n",
    "    \n",
    "    return unified_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('五道口', 'DATE', (9, 12))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('五道口', 'DATE', (9, 12))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"清华大学位于海淀区五道口。\"]\n",
    "pos_sentence = pos_tag_sentences(sentence)\n",
    "ner_sentence = ner_tag_sentences(sentence)\n",
    "find_argument(pos_sentence, ner_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用hueristic信息进行关系词抽取\n",
    "syntactic信息已知，lexical信息需要构建词典获得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关系词抽取\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关系词unify\n",
    "def unify_relation(relations):\n",
    "    unified_relations = []\n",
    "    for i in range(len(relations)):\n",
    "        if relations[i][0] == -1 and relations[i][1] == -1:\n",
    "            continue\n",
    "        for j in range(i+1, len(relations)):\n",
    "            if relations[j][0] == -1 and relations[j][1] == -1:\n",
    "                continue\n",
    "            if (relations[i][1] >= relations[j][0] and relations[i][0] < relations[j][1]) \\\n",
    "                or (relations[j][1] >= relations[i][0] and relations[j][0] < relations[i][1]):\n",
    "                relations[j] = (min(relations[i][0], relations[j][0]), max(relations[i][1], relations[j][1]))\n",
    "                relations[i] = (-1,-1)\n",
    "                break\n",
    "        if relations[i][0] != -1 and relations[i][1] != -1:\n",
    "            unified_relations.append(relations[i])\n",
    "    return unified_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可能的关系模式\n",
    "VERB = \"(RB_pos)?\" + \"(MD_pos|VB_pos|VBD_pos|VBP_pos|VBZ_pos|VBG_pos|VBN_pos)\" + \"(RP_pos)?(RB_pos)?\"\n",
    "WORD = \"(\\$_pos|PRP\\$_pos|CD_pos|DT_pos|JJ_pos|JJS_pos|JJR_pos|NN_pos\" + \"|NNS_pos|NNP_pos|NNPS_pos|POS_pos|PRP_pos|RB_pos|RBR_pos|RBS_pos\" + \"|VBN_pos|VBG_pos)\"\n",
    "PREP = \"(RB_pos)?(IN_pos|TO_pos|RP_pos)(RB_pos)?\"\n",
    "\n",
    "LONG_RELATION_PATTERN = \"(%s(%s*(%s)+)?)+\" % (VERB, WORD, PREP)\n",
    "SHORT_RELATION_PATTERN = \"(%s(%s)?)+\" % (VERB, PREP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文 可能的关系模式\n",
    "VERB = \"(VA_posDEV_pos)?(VC_pos|VE_pos|VV_pos)\"\n",
    "WORD = \"(JJ_pos|VA_pos|DEC_pos|DEG_pos\" + \"|NN_pos|NR_pos|NT_pos\" + \"|M_pos|LC_pos|DEV_pos|DT_pos)\"\n",
    "PREP = \"(VA_posDEV_pos)?(P_pos)(VA_posDEV_pos)?\"\n",
    "\n",
    "LONG_RELATION_PATTERN = \"((%s(%s*)+)?%s)+\" % (PREP, WORD, VERB)\n",
    "SHORT_RELATION_PATTERN = \"((%s(%s)?)+)|(((%s)?%s)+)\" % (VERB, PREP, PREP, VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_long = re.compile(LONG_RELATION_PATTERN)\n",
    "pattern_short = re.compile(SHORT_RELATION_PATTERN)\n",
    "\n",
    "# 从经过pos_tag的句子中，提取所有符合sytactic constraint和lexical constraint的关系词，返回关系词在句中的起止位置\n",
    "def find_relation(processed_sentence, use_lexical, k):\n",
    "    pos_rep = \"\"\n",
    "    position = []\n",
    "    for i,token in enumerate(processed_sentence):\n",
    "        pos_rep += token[1]\n",
    "        for j in range(len(token[1])):\n",
    "            position.append(i)\n",
    "    print(pos_rep)\n",
    "    print(position)\n",
    "    \n",
    "    #pos_rep = \"RB_posVB_posCD_posIN_posRB_posRB_posRB_posVB_posCD_posIN_pos\"\n",
    "    extract_relation = [] # 存储relation在原句子中的位置（第几个词开始，第几个词结束，按照pos_tag分词后的结构计数）,左闭右开区间\n",
    "    # vwp结构检测\n",
    "    m_long = pattern_long.finditer(pos_rep)\n",
    "    for relation in m_long:\n",
    "        relation_start_token = position[relation.start()]\n",
    "        relation_end_token = position[relation.end()-1]\n",
    "        relation_start = processed_sentence[relation_start_token][2][0]\n",
    "        relation_end = processed_sentence[relation_end_token][2][1]\n",
    "        relation_position = (relation_start, relation_end)\n",
    "        extract_relation.append(relation_position)\n",
    "\n",
    "    # vp结构检测\n",
    "    m_short = pattern_short.finditer(pos_rep)\n",
    "    for relation in m_short:\n",
    "        relation_start_token = position[relation.start()]\n",
    "        relation_end_token = position[relation.end()-1]\n",
    "        relation_start = processed_sentence[relation_start_token][2][0]\n",
    "        relation_end = processed_sentence[relation_end_token][2][1]\n",
    "        relation_position = (relation_start, relation_end)\n",
    "        extract_relation.append(relation_position)\n",
    "    \n",
    "    print(extract_relation)\n",
    "    # 判断是否符合lexical constraint\n",
    "    if use_lexical == True:\n",
    "        for index, relation in enumerate(extract_relation):\n",
    "            relation_text = \"\"\n",
    "            for i in range(relation[0], relation[1]):\n",
    "                relation_text += pocessed_sentence[i][0]\n",
    "            if dict_relation_freq[relation_text] < k:\n",
    "                extract_relation[index] = (-1, -1)\n",
    "\n",
    "    # 关系词集合去重\n",
    "    extract_relation = unify_relation(extract_relation)\n",
    "    print(extract_relation)\n",
    "    \n",
    "    return extract_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f9897533b270>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfind_relation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "find_relation(processed_sentences[0], False, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据获得的relation列表和argument列表进行关系组合\n",
    "为每个关系词找到左边和右边最近的argument，构成relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每个relation找到与之对应的argument，返回arg1-rel-arg2结构的列表，表中实体与关系均由文本中的位置表示\n",
    "def find_relation_argument(relations, arguments):\n",
    "    arg1_rel_arg2_pos = []\n",
    "    for relation in relations:\n",
    "        # 找到与relation相距最近的左侧和右侧实体\n",
    "        left_near = (-1,-1)\n",
    "        right_near = (-1,-1)\n",
    "        for argument in arguments:\n",
    "            if argument[0] > left_near[0] and argument[0] < relation[0] and argument[1] <= relation[1]:\n",
    "                left_near = argument\n",
    "            if (right_near == (-1,-1) and argument[1] > relation[1] and argument[0] >= relation[0]) \\\n",
    "                or (argument[1] < right_near[1] and argument[1] > relation[1] and argument[0] >= relation[0]):\n",
    "                right_near = argument\n",
    "        print(relation, left_near, right_near)\n",
    "        \n",
    "        # 如果没有符合要求的实体，则该关系无法找到合适的实体，开始对新的关系进行三元组匹配\n",
    "        if left_near == (-1,-1) or right_near == (-1,-1):\n",
    "            continue\n",
    "        \n",
    "        # 如果实体与关系有交叉部分，则将实体缩短成不交叉的部分\n",
    "        if left_near[1] > relation[0]:\n",
    "            left_near = (left_near[0], relation[0])\n",
    "        if right_near[0] < relation[1]:\n",
    "            right_near = (relation[1], right_near[1])\n",
    "            \n",
    "        arg1_rel_arg2_pos.append((left_near, relation, right_near))\n",
    "    return arg1_rel_arg2_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (2, 4) (6, 7)\n",
      "(8, 9) (6, 7) (10, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((2, 3), (3, 5), (6, 7)), ((6, 7), (8, 9), (10, 11))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations = [(3,5), (8,9)]\n",
    "arguments = [(1,2), (10,11), (2,4), (6,7)]\n",
    "find_relation_argument(relations, arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将relaiton与argument还原为文本，形成文本组成的三元组抽取结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relation_argument_text(sentence, arg1_rel_arg2_pos):\n",
    "    arg1 = \"\"\n",
    "    for i in range(arg1_rel_arg2_pos[0][0], arg1_rel_arg2_pos[0][1]):\n",
    "        arg1 += sentence[i]\n",
    "    \n",
    "    rel = \"\"\n",
    "    for j in range(arg1_rel_arg2_pos[1][0], arg1_rel_arg2_pos[1][1]):\n",
    "        rel += sentence[j]\n",
    "    \n",
    "    arg2 = \"\"\n",
    "    for k in range(arg1_rel_arg2_pos[2][0], arg1_rel_arg2_pos[2][1]):\n",
    "        arg2 += sentence[k]\n",
    "    \n",
    "    extract_triple = {'sentence': sentence, 'arg1':arg1, 'relation': rel, 'arg2': arg2}\n",
    "    return extract_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '奥巴马是美国总统。', 'arg1': '奥巴马', 'relation': '是', 'arg2': '美国总统'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"奥巴马是美国总统。\"\n",
    "arg1_rel_arg2_pos = [(0,3),(3,4),(4,8)]\n",
    "find_relation_argument_text(sentence, arg1_rel_arg2_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用logistic回归对每个三元组的正确概率进行估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log_regression:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(extract_triple):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整体流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('德国', 'NR_pos', (0, 2)), ('总统', 'NN_pos', (2, 4)), ('高克', 'NR_pos', (4, 6)), ('。', 'PU_pos', (6, 7))], [('高克', 'AD_pos', (0, 2)), ('访问', 'VV_pos', (2, 4)), ('中国', 'NR_pos', (4, 6)), ('。', 'PU_pos', (6, 7))], [('习近平', 'NR_pos', (0, 3)), ('在', 'P_pos', (3, 4)), ('上海', 'NR_pos', (4, 6)), ('视察', 'VV_pos', (6, 8)), ('。', 'PU_pos', (8, 9))], [('习近', 'VV_pos', (0, 2)), ('平对', 'NR_pos', (2, 4)), ('埃及', 'NR_pos', (4, 6)), ('进行', 'VV_pos', (6, 8)), ('国事', 'NN_pos', (8, 10)), ('访问', 'NN_pos', (10, 12)), ('。', 'PU_pos', (12, 13))], [('奥巴马', 'NR_pos', (0, 3)), ('毕业于', 'VV_pos', (3, 6)), ('哈佛', 'NR_pos', (6, 8)), ('大学', 'NN_pos', (8, 10)), ('。', 'PU_pos', (10, 11))], [('习近平', 'NR_pos', (0, 3)), ('主席', 'NN_pos', (3, 5)), ('和', 'CC_pos', (5, 6)), ('李克强', 'NR_pos', (6, 9)), ('总理', 'NN_pos', (9, 11)), ('接见', 'VV_pos', (11, 13)), ('普京', 'NR_pos', (13, 15)), ('。', 'PU_pos', (15, 16))], [('习近平', 'NR_pos', (0, 3)), ('访问', 'VV_pos', (3, 5)), ('了', 'AS_pos', (5, 6)), ('美国', 'NR_pos', (6, 8)), ('和', 'CC_pos', (8, 9)), ('英国', 'NR_pos', (9, 11)), ('。', 'PU_pos', (11, 12))], [('高克', 'AD_pos', (0, 2)), ('访问', 'VV_pos', (2, 4)), ('中国', 'NR_pos', (4, 6)), ('，', 'PU_pos', (6, 7)), ('并', 'AD_pos', (7, 8)), ('在', 'P_pos', (8, 9)), ('同济', 'NR_pos', (9, 11)), ('大学', 'NN_pos', (11, 13)), ('发表', 'VV_pos', (13, 15)), ('演讲', 'NN_pos', (15, 17)), ('。', 'PU_pos', (17, 18))]]\n",
      "[[('德国', 'GPE', (0, 2))], [('中国', 'GPE', (4, 6))], [('习近平', 'PERSON', (0, 3)), ('上海', 'GPE', (4, 6))], [('平对埃及', 'GPE', (2, 6))], [('奥巴马', 'EVENT', (0, 3)), ('哈佛大学', 'ORG', (6, 10))], [('习近平', 'PERSON', (0, 3)), ('李克强', 'PERSON', (6, 9)), ('普京', 'PERSON', (13, 15))], [('习近平', 'PERSON', (0, 3)), ('美国', 'GPE', (6, 8)), ('英国', 'GPE', (9, 11))], [('中国', 'GPE', (4, 6)), ('同济大学', 'ORG', (9, 13))]]\n",
      "[(0, 2), (2, 4), (4, 6), (0, 2)]\n",
      "NR_posNN_posNR_posPU_pos\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]\n",
      "[]\n",
      "[]\n",
      "[(4, 6), (4, 6)]\n",
      "AD_posVV_posNR_posPU_pos\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]\n",
      "[(2, 4), (2, 4)]\n",
      "[(2, 4)]\n",
      "(2, 4) (-1, -1) (4, 6)\n",
      "[(0, 3), (4, 6), (0, 3), (4, 6)]\n",
      "NR_posP_posNR_posVV_posPU_pos\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4]\n",
      "[(3, 8), (6, 8)]\n",
      "[(3, 8)]\n",
      "(3, 8) (0, 3) (-1, -1)\n",
      "[(2, 4), (4, 6), (8, 10), (10, 12), (2, 6)]\n",
      "VV_posNR_posNR_posVV_posNN_posNN_posPU_pos\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6]\n",
      "[(0, 2), (6, 8), (0, 2), (6, 8)]\n",
      "[(0, 2), (6, 8)]\n",
      "(0, 2) (-1, -1) (2, 6)\n",
      "(6, 8) (2, 6) (8, 12)\n",
      "[(0, 3), (6, 8), (8, 10), (0, 3), (6, 10)]\n",
      "NR_posVV_posNR_posNN_posPU_pos\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4]\n",
      "[(3, 6), (3, 6)]\n",
      "[(3, 6)]\n",
      "(3, 6) (0, 3) (6, 10)\n",
      "[(0, 3), (3, 5), (6, 9), (9, 11), (13, 15), (0, 3), (6, 9), (13, 15)]\n",
      "NR_posNN_posCC_posNR_posNN_posVV_posNR_posPU_pos\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7]\n",
      "[(11, 13), (11, 13)]\n",
      "[(11, 13)]\n",
      "(11, 13) (6, 11) (13, 15)\n",
      "[(0, 3), (6, 8), (9, 11), (0, 3), (6, 8), (9, 11)]\n",
      "NR_posVV_posAS_posNR_posCC_posNR_posPU_pos\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6]\n",
      "[(3, 5), (3, 5)]\n",
      "[(3, 5)]\n",
      "(3, 5) (0, 3) (6, 8)\n",
      "[(4, 6), (9, 11), (11, 13), (15, 17), (4, 6), (9, 13)]\n",
      "AD_posVV_posNR_posPU_posAD_posP_posNR_posNN_posVV_posNN_posPU_pos\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10]\n",
      "[(2, 4), (8, 15), (2, 4), (13, 15)]\n",
      "[(2, 4), (8, 15)]\n",
      "(2, 4) (-1, -1) (4, 6)\n",
      "(8, 15) (4, 6) (15, 17)\n",
      "[{'sentence': '习近平对埃及进行国事访问。', 'arg1': '平对埃及', 'relation': '进行', 'arg2': '国事访问'}, {'sentence': '奥巴马毕业于哈佛大学。', 'arg1': '奥巴马', 'relation': '毕业于', 'arg2': '哈佛大学'}, {'sentence': '习近平主席和李克强总理接见普京。', 'arg1': '李克强总理', 'relation': '接见', 'arg2': '普京'}, {'sentence': '习近平访问了美国和英国。', 'arg1': '习近平', 'relation': '访问', 'arg2': '美国'}, {'sentence': '高克访问中国，并在同济大学发表演讲。', 'arg1': '中国', 'relation': '在同济大学发表', 'arg2': '演讲'}]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"德国总统高克。\",\"高克访问中国。\",\"习近平在上海视察。\",\"习近平对埃及进行国事访问。\",\"奥巴马毕业于哈佛大学。\",\n",
    "             \"习近平主席和李克强总理接见普京。\",\"习近平访问了美国和英国。\",\"高克访问中国，并在同济大学发表演讲。\"]\n",
    "pos_tag = pos_tag_sentences(sentences)\n",
    "print(pos_tag)\n",
    "ner_tag = ner_tag_sentences(sentences)\n",
    "print(ner_tag)\n",
    "\n",
    "extracted_triple = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    arguments = find_argument(pos_tag[i], ner_tag[i])\n",
    "    relations = find_relation(pos_tag[i], False, 1)\n",
    "    arg1_rel_arg2_pos = find_relation_argument(relations, arguments)\n",
    "    \n",
    "    for triple in arg1_rel_arg2_pos:\n",
    "        text_triple = find_relation_argument_text(sentence, triple)\n",
    "        extracted_triple.append(text_triple)\n",
    "\n",
    "print(extracted_triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('小明', 'NR_pos', (0, 2)),\n",
       "  ('成为', 'VV_pos', (2, 4)),\n",
       "  ('了', 'AS_pos', (4, 5)),\n",
       "  ('第一', 'OD_pos', (5, 7)),\n",
       "  ('个', 'M_pos', (7, 8)),\n",
       "  ('吃', 'VV_pos', (8, 9)),\n",
       "  ('螃蟹', 'NN_pos', (9, 11)),\n",
       "  ('的', 'DEC_pos', (11, 12)),\n",
       "  ('人', 'NN_pos', (12, 13)),\n",
       "  ('。', 'PU_pos', (13, 14))],\n",
       " [('清华', 'NR_pos', (0, 2)),\n",
       "  ('大学', 'NN_pos', (2, 4)),\n",
       "  ('位于', 'VV_pos', (4, 6)),\n",
       "  ('海淀区', 'NR_pos', (6, 9)),\n",
       "  ('五道口', 'NT_pos', (9, 12)),\n",
       "  ('。', 'PU_pos', (12, 13))]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"小明成为了第一个吃螃蟹的人。\",\"清华大学位于海淀区五道口。\"]\n",
    "pos_tag = pos_tag_sentences(sentences)\n",
    "pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
